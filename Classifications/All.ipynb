{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import keras\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data files\n",
    "comments_attack = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations_attack = pd.read_csv('attack_annotations.tsv',  sep = '\\t')\n",
    "comments_aggression = pd.read_csv('aggression_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations_aggression = pd.read_csv('aggression_annotations.tsv',  sep = '\\t')\n",
    "comments_toxicity = pd.read_csv('toxicity_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations_toxicity = pd.read_csv('toxicity_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels a comment if the majority of annoatators did so\n",
    "labels_attack = annotations_attack.groupby('rev_id')['attack'].mean() > 0.5\n",
    "labels_aggression = annotations_aggression.groupby('rev_id')['aggression'].mean() > 0.5\n",
    "labels_toxicity = annotations_toxicity.groupby('rev_id')['toxicity'].mean() > 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments_attack['label'] = labels_attack\n",
    "comments_aggression['label'] = labels_aggression\n",
    "comments_toxicity['label'] = labels_toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only Attack, Aggression, Toxicity\n",
    "comments_attack = comments_attack.query(\"label == True\")\n",
    "comments_aggression = comments_aggression.query(\"label == True\")\n",
    "comments_toxicity = comments_toxicity.query(\"label == True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels: Attack = 0, Aggression =1, Toxicity = 2\n",
    "comments_attack['label'] = comments_attack['label']*0\n",
    "comments_aggression['label'] = comments_aggression['label']*1\n",
    "comments_toxicity['label'] = comments_toxicity['label']*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13590"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation of the three data sets\n",
    "dataframe = pd.concat([comments_attack, comments_aggression, comments_toxicity], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.lower())\n",
    "dataframe['comment'] = dataframe['comment'].apply((lambda x: re.sub('[/(){}\\[\\]\\|@,;]','',x)))\n",
    "dataframe['comment'] = dataframe['comment'].apply((lambda x: re.sub('[^0-9a-z #+_]','',x)))\n",
    "dataframe['comment'] = dataframe['comment'].apply((lambda x: re.sub('[^0-9a-z #+_]','',x)))\n",
    "dataframe['comment'] = dataframe['comment'].apply((lambda x: re.sub(' +',' ',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to pull their heads from their collective arses'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.iloc[1002]['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and validation sets\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(dataframe['comment'], dataframe['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train comments length:  34987\n",
      "test comments length:  8747\n"
     ]
    }
   ],
   "source": [
    "print('train comments length: ',len(train_x))\n",
    "print('test comments length: ',len(valid_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'.')\n",
    "count_vect.fit(dataframe['comment'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(dataframe['comment'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,4), max_features=5000)\n",
    "tfidf_vect_ngram.fit(dataframe['comment'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(dataframe['comment'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec', encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(dataframe['comment'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, xtrain, ytrain, xvalid, yvalid):\n",
    "    \n",
    "\n",
    "   \n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(xvalid)\n",
    "        \n",
    "    accuracy = metrics.accuracy_score(predictions, yvalid)\n",
    "    f1score = metrics.f1_score(yvalid, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:   accuracy: 0.3437750085743684      f1 score: 0.3118598812636129\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count, valid_y)\n",
    "print(\"NB, Count Vectors:   accuracy: %s      f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:   accuracy: 0.3437750085743684      f1 score: 0.3118598812636129\n",
      "NB, WordLevel TF-IDF:   accuracy: 0.2560878015319538     f1 score: 0.22216247424119906\n",
      "NB, N-Gram Vectors:   accuracy: 0.2816965816851492     f1 score: 0.23907910356412748\n",
      "NB, CharLevel Vectors:   accuracy: 0.2981593689264891   f1 score: 0.26497631292940804\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count, valid_y)\n",
    "print(\"NB, Count Vectors:   accuracy: %s      f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"NB, WordLevel TF-IDF:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print(\"NB, N-Gram Vectors:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"NB, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silver\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Silver\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Silver\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:   accuracy: 0.33131359323196524   f1 score: 0.17108206844035737\n",
      "LR, WordLevel TF-IDF:   accuracy: 0.22510575054304333   f1 score: 0.21383260886750863\n",
      "LR, N-Gram Vectors:   accuracy: 0.26329027095004004   f1 score: 0.23339983237368095\n",
      "LR, CharLevel Vectors:   accuracy: 0.24111123813879043   f1 score: 0.22624874099899567\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count, valid_y)\n",
    "print(\"LR, Count Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"LR, WordLevel TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print(\"LR, N-Gram Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"LR, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silver\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors:   accuracy: 0.13799016805761977   f1 score: 0.12199894437396669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silver\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF:   accuracy: 0.3528066765748257   f1 score: 0.18402119562408722\n",
      "SVM, N-Gram Vectors TF-IDF:   accuracy: 0.3528066765748257   f1 score: 0.18402119562408722\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier on Count Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count,valid_y)\n",
    "print(\"SVM, Count Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# SVM Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"SVM, WordLevel TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print(\"SVM, N-Gram Vectors TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# SVM Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"SVM, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward NN with 1 hidden layer\n",
    "def model_FF(xtrain, ytrain, xvalid, yvalid, hidden_size, epochs =1):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((xtrain.shape[1], ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(hidden_size, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(3, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "    classifier.fit(xtrain, ytrain,\n",
    "                  batch_size=256,\n",
    "                  epochs=epochs,\n",
    "                  shuffle = True)\n",
    "    # scores of the classifier\n",
    "    predictions = classifier.predict(xvalid)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = classifier.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to one_hot\n",
    "train_y_onehot = keras.utils.to_categorical(train_y, 3)\n",
    "valid_y_onehot = keras.utils.to_categorical(valid_y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "34987/34987 [==============================] - 2s 64us/step - loss: 1.6664 - acc: 0.3321\n",
      "NN, Count Vectors accuracy:0.32193895051094196     f1 score: 0.32204448781604045\n",
      "Epoch 1/1\n",
      "34987/34987 [==============================] - 4s 127us/step - loss: 1.0998 - acc: 0.3299\n",
      "NN, Count Vectors accuracy:0.32674059678625894     f1 score: 0.25234666100189335\n",
      "Epoch 1/1\n",
      "34987/34987 [==============================] - 4s 114us/step - loss: 1.1002 - acc: 0.3288\n",
      "NN, Count Vectors accuracy:0.3318852177991777     f1 score: 0.21099053638215218\n",
      "Epoch 1/1\n",
      "34987/34987 [==============================] - 6s 185us/step - loss: 1.0988 - acc: 0.3448\n",
      "NN, Count Vectors accuracy:0.3528066765918614     f1 score: 0.18402119562408722\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NN Classifier on Count Vectors\n",
    "accuracy, f1score = model_FF(xtrain_count, train_y_onehot, xvalid_count, valid_y_onehot, 100)\n",
    "print(\"NN, Count Vectors accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))\n",
    "\n",
    "# NN Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1score = model_FF(xtrain_tfidf, train_y_onehot, xvalid_tfidf, valid_y_onehot, 100)\n",
    "print(\"NN, Count Vectors accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))\n",
    "\n",
    "# NN Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = model_FF(xtrain_tfidf_ngram, train_y_onehot, xvalid_tfidf_ngram, valid_y_onehot, 100)\n",
    "print(\"NN, Count Vectors accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))\n",
    "\n",
    "# NN Classifier on Count Vectors\n",
    "accuracy, f1score = model_FF(xtrain_tfidf_ngram_chars, train_y_onehot, xvalid_tfidf_ngram_chars, valid_y_onehot, 100)\n",
    "print(\"NN, Count Vectors accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(xtrain, ytrain, xvalid, yvalid, epochs = 3):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=epochs)\n",
    "    predictions = model.predict(xvalid)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "34987/34987 [==============================] - 51s 1ms/step - loss: 1.0993 - acc: 0.3440\n",
      "Epoch 2/3\n",
      "34987/34987 [==============================] - 47s 1ms/step - loss: 1.0971 - acc: 0.3544\n",
      "Epoch 3/3\n",
      "34987/34987 [==============================] - 47s 1ms/step - loss: 1.0968 - acc: 0.3555\n",
      "CNN, Word Embeddings acuuracy accuracy:0.3506345032752957     f1 score: 0.18730536879517634\n"
     ]
    }
   ],
   "source": [
    "accuracy, f1score = cnn(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"CNN, Word Embeddings acuuracy accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(xtrain, ytrain, xvalid, yvalid, epochs = 1):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer1 = layers.LSTM(128)(embedding_layer)\n",
    "    dropout1 = layers.Dropout(0.5)(lstm_layer1)\n",
    "    #lstm_layer2 = layers.LSTM(128)(dropout1)\n",
    "    #dropout2 = layers.Dropout(0.5)(lstm_layer2)\n",
    "    # Add the output Layers\n",
    "    output_layer = layers.Dense(3, activation=\"softmax\")(dropout1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=3)\n",
    "    \n",
    "    predictions = model.predict(xvalid)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "34987/34987 [==============================] - 195s 6ms/step - loss: 1.1013 - acc: 0.3416\n",
      "Epoch 2/3\n",
      "34987/34987 [==============================] - 215s 6ms/step - loss: 1.0987 - acc: 0.3510\n",
      "Epoch 3/3\n",
      "34987/34987 [==============================] - 208s 6ms/step - loss: 1.0978 - acc: 0.3552\n",
      "LSTM, Word Embeddings accuracy:0.3434320338503952     f1 score: 0.2693006603666671\n"
     ]
    }
   ],
   "source": [
    "accuracy, f1score = lstm(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"LSTM, Word Embeddings accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
