{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data files\n",
    "comments_attack = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations_attack = pd.read_csv('attack_annotations.tsv',  sep = '\\t')\n",
    "comments_aggression = pd.read_csv('aggression_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations_aggression = pd.read_csv('aggression_annotations.tsv',  sep = '\\t')\n",
    "comments_toxicity = pd.read_csv('toxicity_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations_toxicity = pd.read_csv('toxicity_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels a comment if the majority of annoatators did so\n",
    "labels_attack = annotations_attack.groupby('rev_id')['attack'].mean() > 0.5\n",
    "labels_aggression = annotations_aggression.groupby('rev_id')['aggression'].mean() > 0.5\n",
    "labels_toxicity = annotations_toxicity.groupby('rev_id')['toxicity'].mean() > 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "37675        False\n",
       "44816        False\n",
       "49851        False\n",
       "89320        False\n",
       "93890        False\n",
       "102817       False\n",
       "103624       False\n",
       "111032       False\n",
       "120283       False\n",
       "128532       False\n",
       "133562       False\n",
       "138117       False\n",
       "155243       False\n",
       "177310       False\n",
       "192579       False\n",
       "201190       False\n",
       "208009       False\n",
       "249432       False\n",
       "252031       False\n",
       "268558       False\n",
       "276906       False\n",
       "286174       False\n",
       "290598       False\n",
       "294124       False\n",
       "297866       False\n",
       "317177       False\n",
       "336654       False\n",
       "344567       False\n",
       "356383       False\n",
       "358984       False\n",
       "             ...  \n",
       "699646005     True\n",
       "699659494     True\n",
       "699660419     True\n",
       "699661020     True\n",
       "699661834    False\n",
       "699663770    False\n",
       "699664687     True\n",
       "699667660    False\n",
       "699683891    False\n",
       "699698850    False\n",
       "699702006    False\n",
       "699703322    False\n",
       "699715740    False\n",
       "699728036    False\n",
       "699730832    False\n",
       "699732149    False\n",
       "699741197    False\n",
       "699753082    False\n",
       "699755057    False\n",
       "699756053    False\n",
       "699756185    False\n",
       "699780538    False\n",
       "699813325    False\n",
       "699820699    False\n",
       "699822249    False\n",
       "699848324    False\n",
       "699851288    False\n",
       "699857133    False\n",
       "699891012    False\n",
       "699897151    False\n",
       "Name: aggression, Length: 115864, dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_aggression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments_attack['label'] = labels_attack*1.0\n",
    "comments_aggression['label'] = labels_aggression*2.0\n",
    "comments_toxicity['label'] = labels_toxicity*3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "37675        0.0\n",
       "44816        0.0\n",
       "49851        0.0\n",
       "89320        0.0\n",
       "93890        0.0\n",
       "102817       0.0\n",
       "103624       0.0\n",
       "111032       0.0\n",
       "120283       0.0\n",
       "128532       0.0\n",
       "133562       0.0\n",
       "138117       0.0\n",
       "155243       0.0\n",
       "177310       0.0\n",
       "192579       0.0\n",
       "201190       0.0\n",
       "208009       0.0\n",
       "249432       0.0\n",
       "252031       0.0\n",
       "268558       0.0\n",
       "276906       0.0\n",
       "286174       0.0\n",
       "290598       0.0\n",
       "294124       0.0\n",
       "297866       0.0\n",
       "317177       0.0\n",
       "336654       0.0\n",
       "344567       0.0\n",
       "356383       0.0\n",
       "358984       0.0\n",
       "            ... \n",
       "699646005    2.0\n",
       "699659494    2.0\n",
       "699660419    2.0\n",
       "699661020    2.0\n",
       "699661834    0.0\n",
       "699663770    0.0\n",
       "699664687    2.0\n",
       "699667660    0.0\n",
       "699683891    0.0\n",
       "699698850    0.0\n",
       "699702006    0.0\n",
       "699703322    0.0\n",
       "699715740    0.0\n",
       "699728036    0.0\n",
       "699730832    0.0\n",
       "699732149    0.0\n",
       "699741197    0.0\n",
       "699753082    0.0\n",
       "699755057    0.0\n",
       "699756053    0.0\n",
       "699756185    0.0\n",
       "699780538    0.0\n",
       "699813325    0.0\n",
       "699820699    0.0\n",
       "699822249    0.0\n",
       "699848324    0.0\n",
       "699851288    0.0\n",
       "699857133    0.0\n",
       "699891012    0.0\n",
       "699897151    0.0\n",
       "Name: label, Length: 115864, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_aggression['label']\n",
    "699646005    2.0\n",
    "699659494    2.0\n",
    "699660419    2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_attack = comments_attack.query(\"label == 1.0\")\n",
    "comments_aggression = comments_aggression.query(\"label == 2.0\")\n",
    "comments_toxicity= comments_toxicity.query(\"label == 3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>801279.0</th>\n",
       "      <td>Iraq is not good  ===NEWLINE_TOKENNEWLINE_TO...</td>\n",
       "      <td>2003</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702703.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN____NEWLINE_TOKENfuc...</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632658.0</th>\n",
       "      <td>i have a dick, its bigger than yours! hahaha</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6545332.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== renault ==NEWLINE...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6545351.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== renault ==NEWLINE...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7977970.0</th>\n",
       "      <td>34, 30 Nov 2004 (UTC)NEWLINE_TOKENNEWLINE_TOKE...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8359431.0</th>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN::You are not worth...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8724028.0</th>\n",
       "      <td>Yes, complain to your rabbi and then go shoot ...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8845700.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENi am using the sandb...</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8845736.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== GOD DAMN ==NEWLIN...</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9392747.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== WHY ARE YOU SUCH ...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9396830.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== I HAVE A HARD ON ...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9521926.0</th>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKEN===an ...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9664203.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== whoa ==NEWLINE_TO...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9664254.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== whoa ==NEWLINE_TO...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9679297.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== FUCK YOU THUE ==N...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9725692.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN::: Hey Bobby, get a...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10144987.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN:: First of all, who...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10253580.0</th>\n",
       "      <td>Hey  Kerry is a loser and so is Gore.- NEWLINE...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10255042.0</th>\n",
       "      <td>Wait a minute.  I looked at your background an...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266382.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENLOL; charles, you ar...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266787.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENWOW MIRV IS TEH prot...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266890.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== SHUT UP, FAG ==NE...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10335102.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN==Ah What?==NEWLINE_...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10900894.0</th>\n",
       "      <td>I also found that this guy names Sever is lik...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10949158.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENYeah, what the fuck ...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952727.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENFUCK YOU ALKIVA wiki...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952841.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== what to do with e...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11121273.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKEN==Can I...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11121620.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKEN==Thank...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698337564.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== Scum of the earth...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698353733.0</th>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN==  ==NEWLINE_TOKEN...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698389664.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENWhy don't you get a ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698403592.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENfoot fetishes are aw...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698416945.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== I'm back!!! ==NEW...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698493321.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== This is for remov...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698590313.0</th>\n",
       "      <td>were NOT vandalism you idiot!!!</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698705636.0</th>\n",
       "      <td>NEWLINE_TOKEN:::::Once again.... I'm SO glad t...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698712715.0</th>\n",
       "      <td>NEWLINE_TOKENHoly shit, you people suck. I don...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698763771.0</th>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN== Who do you think...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698873916.0</th>\n",
       "      <td>STOP your propaganda on getting me blocked fo...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698934733.0</th>\n",
       "      <td>`NEWLINE_TOKEN:::LOL, rest assured there is no...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698984149.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENWell,Go...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699129241.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENyou bitch why does i...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699163319.0</th>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN== Go fuck yourself...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699349503.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN@ okay king of the W...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699355719.0</th>\n",
       "      <td>`NEWLINE_TOKEN:Thank you. Given the misuse of ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699445326.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENHey Stupydbytch Frod...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699445375.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENDelete my 200 articl...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699447117.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENYou must be an uglyb...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699549435.0</th>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKENMotherfucka the who...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699569747.0</th>\n",
       "      <td>`NEWLINE_TOKENNo, you utter RETARD! HISPANIC M...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699603533.0</th>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN==``The Returds Nee...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699604332.0</th>\n",
       "      <td>`GET A LIFE DHASS HOLE!== NEWLINE_TOKENFinding...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699645524.0</th>\n",
       "      <td>Brandon Semenuk has won the event four times ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699659494.0</th>\n",
       "      <td>im soory since when is google images not allow...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699660419.0</th>\n",
       "      <td>what ever you fuggin fagNEWLINE_TOKENQuestion ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699661020.0</th>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKEN== Nice try but no c...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699661834.0</th>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN== kys ==NEWLINE_TO...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699664687.0</th>\n",
       "      <td>shut up mind your own business and go fuck so...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43734 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment  year  \\\n",
       "rev_id                                                                 \n",
       "801279.0       Iraq is not good  ===NEWLINE_TOKENNEWLINE_TO...  2003   \n",
       "2702703.0    NEWLINE_TOKENNEWLINE_TOKEN____NEWLINE_TOKENfuc...  2004   \n",
       "4632658.0         i have a dick, its bigger than yours! hahaha  2004   \n",
       "6545332.0    NEWLINE_TOKENNEWLINE_TOKEN== renault ==NEWLINE...  2004   \n",
       "6545351.0    NEWLINE_TOKENNEWLINE_TOKEN== renault ==NEWLINE...  2004   \n",
       "7977970.0    34, 30 Nov 2004 (UTC)NEWLINE_TOKENNEWLINE_TOKE...  2004   \n",
       "8359431.0    `NEWLINE_TOKENNEWLINE_TOKEN::You are not worth...  2004   \n",
       "8724028.0    Yes, complain to your rabbi and then go shoot ...  2004   \n",
       "8845700.0    NEWLINE_TOKENNEWLINE_TOKENi am using the sandb...  2004   \n",
       "8845736.0    NEWLINE_TOKENNEWLINE_TOKEN== GOD DAMN ==NEWLIN...  2004   \n",
       "9392747.0    NEWLINE_TOKENNEWLINE_TOKEN== WHY ARE YOU SUCH ...  2005   \n",
       "9396830.0    NEWLINE_TOKENNEWLINE_TOKEN== I HAVE A HARD ON ...  2005   \n",
       "9521926.0    `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKEN===an ...  2005   \n",
       "9664203.0    NEWLINE_TOKENNEWLINE_TOKEN== whoa ==NEWLINE_TO...  2005   \n",
       "9664254.0    NEWLINE_TOKENNEWLINE_TOKEN== whoa ==NEWLINE_TO...  2005   \n",
       "9679297.0    NEWLINE_TOKENNEWLINE_TOKEN== FUCK YOU THUE ==N...  2005   \n",
       "9725692.0    NEWLINE_TOKENNEWLINE_TOKEN::: Hey Bobby, get a...  2005   \n",
       "10144987.0   NEWLINE_TOKENNEWLINE_TOKEN:: First of all, who...  2005   \n",
       "10253580.0   Hey  Kerry is a loser and so is Gore.- NEWLINE...  2005   \n",
       "10255042.0   Wait a minute.  I looked at your background an...  2005   \n",
       "10266382.0   NEWLINE_TOKENNEWLINE_TOKENLOL; charles, you ar...  2005   \n",
       "10266787.0   NEWLINE_TOKENNEWLINE_TOKENWOW MIRV IS TEH prot...  2005   \n",
       "10266890.0   NEWLINE_TOKENNEWLINE_TOKEN== SHUT UP, FAG ==NE...  2005   \n",
       "10335102.0   NEWLINE_TOKENNEWLINE_TOKEN==Ah What?==NEWLINE_...  2005   \n",
       "10900894.0    I also found that this guy names Sever is lik...  2005   \n",
       "10949158.0   NEWLINE_TOKENNEWLINE_TOKENYeah, what the fuck ...  2005   \n",
       "10952727.0   NEWLINE_TOKENNEWLINE_TOKENFUCK YOU ALKIVA wiki...  2005   \n",
       "10952841.0   NEWLINE_TOKENNEWLINE_TOKEN== what to do with e...  2005   \n",
       "11121273.0   NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKEN==Can I...  2005   \n",
       "11121620.0   NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKEN==Thank...  2005   \n",
       "...                                                        ...   ...   \n",
       "698337564.0  NEWLINE_TOKENNEWLINE_TOKEN== Scum of the earth...  2016   \n",
       "698353733.0  `NEWLINE_TOKENNEWLINE_TOKEN==  ==NEWLINE_TOKEN...  2016   \n",
       "698389664.0  NEWLINE_TOKENNEWLINE_TOKENWhy don't you get a ...  2016   \n",
       "698403592.0  NEWLINE_TOKENNEWLINE_TOKENfoot fetishes are aw...  2016   \n",
       "698416945.0  NEWLINE_TOKENNEWLINE_TOKEN== I'm back!!! ==NEW...  2016   \n",
       "698493321.0  NEWLINE_TOKENNEWLINE_TOKEN== This is for remov...  2016   \n",
       "698590313.0                    were NOT vandalism you idiot!!!  2016   \n",
       "698705636.0  NEWLINE_TOKEN:::::Once again.... I'm SO glad t...  2016   \n",
       "698712715.0  NEWLINE_TOKENHoly shit, you people suck. I don...  2016   \n",
       "698763771.0  `NEWLINE_TOKENNEWLINE_TOKEN== Who do you think...  2016   \n",
       "698873916.0   STOP your propaganda on getting me blocked fo...  2016   \n",
       "698934733.0  `NEWLINE_TOKEN:::LOL, rest assured there is no...  2016   \n",
       "698984149.0  NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENWell,Go...  2016   \n",
       "699129241.0  NEWLINE_TOKENNEWLINE_TOKENyou bitch why does i...  2016   \n",
       "699163319.0  `NEWLINE_TOKENNEWLINE_TOKEN== Go fuck yourself...  2016   \n",
       "699349503.0  NEWLINE_TOKENNEWLINE_TOKEN@ okay king of the W...  2016   \n",
       "699355719.0  `NEWLINE_TOKEN:Thank you. Given the misuse of ...  2016   \n",
       "699445326.0  NEWLINE_TOKENNEWLINE_TOKENHey Stupydbytch Frod...  2016   \n",
       "699445375.0  NEWLINE_TOKENNEWLINE_TOKENDelete my 200 articl...  2016   \n",
       "699447117.0  NEWLINE_TOKENNEWLINE_TOKENYou must be an uglyb...  2016   \n",
       "699549435.0  `NEWLINE_TOKENNEWLINE_TOKENMotherfucka the who...  2016   \n",
       "699569747.0  `NEWLINE_TOKENNo, you utter RETARD! HISPANIC M...  2016   \n",
       "699603533.0  `NEWLINE_TOKENNEWLINE_TOKEN==``The Returds Nee...  2016   \n",
       "699604332.0  `GET A LIFE DHASS HOLE!== NEWLINE_TOKENFinding...  2016   \n",
       "699645524.0   Brandon Semenuk has won the event four times ...  2016   \n",
       "699659494.0  im soory since when is google images not allow...  2016   \n",
       "699660419.0  what ever you fuggin fagNEWLINE_TOKENQuestion ...  2016   \n",
       "699661020.0  NEWLINE_TOKENNEWLINE_TOKEN== Nice try but no c...  2016   \n",
       "699661834.0  `NEWLINE_TOKENNEWLINE_TOKEN== kys ==NEWLINE_TO...  2016   \n",
       "699664687.0   shut up mind your own business and go fuck so...  2016   \n",
       "\n",
       "             logged_in       ns   sample  split  label  \n",
       "rev_id                                                  \n",
       "801279.0         False  article   random  train    1.0  \n",
       "2702703.0        False     user   random  train    1.0  \n",
       "4632658.0        False  article  blocked  train    1.0  \n",
       "6545332.0         True     user  blocked  train    1.0  \n",
       "6545351.0         True     user  blocked   test    1.0  \n",
       "7977970.0         True  article   random  train    1.0  \n",
       "8359431.0         True     user  blocked  train    1.0  \n",
       "8724028.0         True     user  blocked   test    1.0  \n",
       "8845700.0        False     user  blocked  train    1.0  \n",
       "8845736.0        False     user  blocked    dev    1.0  \n",
       "9392747.0         True     user  blocked  train    1.0  \n",
       "9396830.0         True     user  blocked    dev    1.0  \n",
       "9521926.0         True     user  blocked    dev    1.0  \n",
       "9664203.0        False     user  blocked  train    1.0  \n",
       "9664254.0        False     user  blocked   test    1.0  \n",
       "9679297.0         True     user  blocked  train    1.0  \n",
       "9725692.0         True     user  blocked  train    1.0  \n",
       "10144987.0        True     user  blocked  train    1.0  \n",
       "10253580.0        True  article  blocked  train    1.0  \n",
       "10255042.0        True  article  blocked  train    1.0  \n",
       "10266382.0       False  article  blocked  train    1.0  \n",
       "10266787.0       False  article  blocked  train    1.0  \n",
       "10266890.0       False     user  blocked  train    1.0  \n",
       "10335102.0       False     user  blocked  train    1.0  \n",
       "10900894.0       False  article  blocked  train    1.0  \n",
       "10949158.0        True     user  blocked  train    1.0  \n",
       "10952727.0       False     user  blocked  train    1.0  \n",
       "10952841.0       False  article  blocked    dev    1.0  \n",
       "11121273.0        True     user  blocked    dev    1.0  \n",
       "11121620.0        True     user  blocked  train    1.0  \n",
       "...                ...      ...      ...    ...    ...  \n",
       "698337564.0      False     user  blocked  train    3.0  \n",
       "698353733.0       True     user   random    dev    3.0  \n",
       "698389664.0      False     user  blocked    dev    3.0  \n",
       "698403592.0      False     user  blocked    dev    3.0  \n",
       "698416945.0      False     user  blocked  train    3.0  \n",
       "698493321.0      False     user  blocked   test    3.0  \n",
       "698590313.0      False     user   random  train    3.0  \n",
       "698705636.0       True  article  blocked  train    3.0  \n",
       "698712715.0       True     user  blocked   test    3.0  \n",
       "698763771.0      False     user  blocked  train    3.0  \n",
       "698873916.0       True     user  blocked  train    3.0  \n",
       "698934733.0       True  article  blocked    dev    3.0  \n",
       "698984149.0      False     user  blocked   test    3.0  \n",
       "699129241.0      False     user  blocked    dev    3.0  \n",
       "699163319.0       True     user  blocked   test    3.0  \n",
       "699349503.0       True     user  blocked  train    3.0  \n",
       "699355719.0       True     user  blocked   test    3.0  \n",
       "699445326.0      False     user  blocked    dev    3.0  \n",
       "699445375.0      False     user  blocked  train    3.0  \n",
       "699447117.0      False     user  blocked  train    3.0  \n",
       "699549435.0       True     user  blocked    dev    3.0  \n",
       "699569747.0      False  article  blocked   test    3.0  \n",
       "699603533.0      False  article  blocked  train    3.0  \n",
       "699604332.0      False     user  blocked    dev    3.0  \n",
       "699645524.0       True     user  blocked    dev    3.0  \n",
       "699659494.0       True     user  blocked   test    3.0  \n",
       "699660419.0       True     user  blocked  train    3.0  \n",
       "699661020.0       True     user  blocked  train    3.0  \n",
       "699661834.0       True     user  blocked  train    3.0  \n",
       "699664687.0       True     user  blocked    dev    3.0  \n",
       "\n",
       "[43734 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenation of the three data sets\n",
    "dataframe = pd.concat([comments_attack, comments_aggression, comments_toxicity], axis = 0)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove newline and tab tokens\n",
    "import re\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.lower())\n",
    "dataframe['comment'] = dataframe['comment'].apply((lambda x: re.sub('.,[^a-zA-z0-9\\s]','',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>801279.0</th>\n",
       "      <td>iraq is not good      usa is bad</td>\n",
       "      <td>2003</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702703.0</th>\n",
       "      <td>____ fuck off you little asshole if you want...</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632658.0</th>\n",
       "      <td>i have a dick its bigger than yours hahaha</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6545332.0</th>\n",
       "      <td>renault   you sad little bpy for driving a ...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6545351.0</th>\n",
       "      <td>renault   you sad little bo for driving a r...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7977970.0</th>\n",
       "      <td>34 30 nov 2004 utc  because you like to accuse...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8359431.0</th>\n",
       "      <td>`  you are not worth the effort you are arguin...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8724028.0</th>\n",
       "      <td>yes complain to your rabbi and then go shoot s...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8845700.0</th>\n",
       "      <td>i am using the sandbox ass wipe</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8845736.0</th>\n",
       "      <td>god damn   god damn it fuckers i am using t...</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9392747.0</th>\n",
       "      <td>why are you such a gay nigger   god damndd ...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9396830.0</th>\n",
       "      <td>i have a hard on for war   sup niggs1111111...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9521926.0</th>\n",
       "      <td>`   an odd deletion  i am curious why did you ...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9664203.0</th>\n",
       "      <td>whoa   you are a bi fat idiot stop spamming...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9664254.0</th>\n",
       "      <td>whoa   you are a big fat idot stop spamming...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9679297.0</th>\n",
       "      <td>fuck you thue   you brain dead fuck you can...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9725692.0</th>\n",
       "      <td>hey bobby get a clue i dont deal with tards...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10144987.0</th>\n",
       "      <td>first of all who the fuck died and made you...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10253580.0</th>\n",
       "      <td>hey  kerry is a loser and so is gore</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10255042.0</th>\n",
       "      <td>wait a minute  i looked at your background and...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266382.0</th>\n",
       "      <td>lol charles you are a real fag arent you sid...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266787.0</th>\n",
       "      <td>wow mirv is teh protect this paeg  lol charl...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266890.0</th>\n",
       "      <td>shut up fag   lol you will ban me huh ooh i...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10335102.0</th>\n",
       "      <td>ah what  you gotta girlfriend the pedophile ...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10900894.0</th>\n",
       "      <td>i also found that this guy names sever is lik...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10949158.0</th>\n",
       "      <td>yeah what the fuck is this get your lazy com...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952727.0</th>\n",
       "      <td>fuck you alkiva wikipedia is a democracy a w...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952841.0</th>\n",
       "      <td>what to do with elitist assholes who do not...</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11121273.0</th>\n",
       "      <td>can i eye fuck you you negro see grow</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11121620.0</th>\n",
       "      <td>thank you your the wellcome arabic assholr ...</td>\n",
       "      <td>2005</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698337564.0</th>\n",
       "      <td>scum of the earth vandal   dont change my p...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698353733.0</th>\n",
       "      <td>`      hello ymblanter   i have warned  severa...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698389664.0</th>\n",
       "      <td>why dont you get a life you sicko fucking twat</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698403592.0</th>\n",
       "      <td>foot fetishes are awesome fuck you</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698416945.0</th>\n",
       "      <td>im back   im back and i fully intend to con...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698493321.0</th>\n",
       "      <td>this is for removing my post on 100   im go...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698590313.0</th>\n",
       "      <td>were not vandalism you idiot</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698705636.0</th>\n",
       "      <td>once again im so glad that wikipedia is not b...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698712715.0</th>\n",
       "      <td>holy shit you people suck i dont see any warn...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698763771.0</th>\n",
       "      <td>`   who do you think you are   who died and ma...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698873916.0</th>\n",
       "      <td>stop your propaganda on getting me blocked fo...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698934733.0</th>\n",
       "      <td>` lol rest assured there is nothing you can ma...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698984149.0</th>\n",
       "      <td>wellgoodbuy isis terroristyou will be block...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699129241.0</th>\n",
       "      <td>you bitch why does it matter its the same go...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699163319.0</th>\n",
       "      <td>`   go fuck yourself    style``backgroundcolor...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699349503.0</th>\n",
       "      <td>okay king of the wikipedia nazis</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699355719.0</th>\n",
       "      <td>` thank you given the misuse of tools here and...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699445326.0</th>\n",
       "      <td>hey stupydbytch frodesiak i am back</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699445375.0</th>\n",
       "      <td>delete my 200 articles whhore</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699447117.0</th>\n",
       "      <td>you must be an uglybytch</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699549435.0</th>\n",
       "      <td>`  motherfucka the whole ``blanqueamento in pu...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699569747.0</th>\n",
       "      <td>` no you utter retard hispanic means from spai...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699603533.0</th>\n",
       "      <td>`  ``the returds need to get a life`` instead ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699604332.0</th>\n",
       "      <td>`get a life dhass hole  finding articles 10min...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699645524.0</th>\n",
       "      <td>brandon semenuk has won the event four times ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699659494.0</th>\n",
       "      <td>im soory since when is google images not allow...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699660419.0</th>\n",
       "      <td>what ever you fuggin fag question how did you ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699661020.0</th>\n",
       "      <td>nice try but no cigaridiot   then explain o...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699661834.0</th>\n",
       "      <td>`   kys    style``backgroundcolor fdffe7 borde...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699664687.0</th>\n",
       "      <td>shut up mind your own business and go fuck so...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43734 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment  year  \\\n",
       "rev_id                                                                 \n",
       "801279.0                   iraq is not good      usa is bad     2003   \n",
       "2702703.0      ____ fuck off you little asshole if you want...  2004   \n",
       "4632658.0           i have a dick its bigger than yours hahaha  2004   \n",
       "6545332.0       renault   you sad little bpy for driving a ...  2004   \n",
       "6545351.0       renault   you sad little bo for driving a r...  2004   \n",
       "7977970.0    34 30 nov 2004 utc  because you like to accuse...  2004   \n",
       "8359431.0    `  you are not worth the effort you are arguin...  2004   \n",
       "8724028.0    yes complain to your rabbi and then go shoot s...  2004   \n",
       "8845700.0                      i am using the sandbox ass wipe  2004   \n",
       "8845736.0       god damn   god damn it fuckers i am using t...  2004   \n",
       "9392747.0       why are you such a gay nigger   god damndd ...  2005   \n",
       "9396830.0       i have a hard on for war   sup niggs1111111...  2005   \n",
       "9521926.0    `   an odd deletion  i am curious why did you ...  2005   \n",
       "9664203.0       whoa   you are a bi fat idiot stop spamming...  2005   \n",
       "9664254.0       whoa   you are a big fat idot stop spamming...  2005   \n",
       "9679297.0       fuck you thue   you brain dead fuck you can...  2005   \n",
       "9725692.0       hey bobby get a clue i dont deal with tards...  2005   \n",
       "10144987.0      first of all who the fuck died and made you...  2005   \n",
       "10253580.0             hey  kerry is a loser and so is gore     2005   \n",
       "10255042.0   wait a minute  i looked at your background and...  2005   \n",
       "10266382.0     lol charles you are a real fag arent you sid...  2005   \n",
       "10266787.0     wow mirv is teh protect this paeg  lol charl...  2005   \n",
       "10266890.0      shut up fag   lol you will ban me huh ooh i...  2005   \n",
       "10335102.0     ah what  you gotta girlfriend the pedophile ...  2005   \n",
       "10900894.0    i also found that this guy names sever is lik...  2005   \n",
       "10949158.0     yeah what the fuck is this get your lazy com...  2005   \n",
       "10952727.0     fuck you alkiva wikipedia is a democracy a w...  2005   \n",
       "10952841.0      what to do with elitist assholes who do not...  2005   \n",
       "11121273.0             can i eye fuck you you negro see grow    2005   \n",
       "11121620.0      thank you your the wellcome arabic assholr ...  2005   \n",
       "...                                                        ...   ...   \n",
       "698337564.0     scum of the earth vandal   dont change my p...  2016   \n",
       "698353733.0  `      hello ymblanter   i have warned  severa...  2016   \n",
       "698389664.0     why dont you get a life you sicko fucking twat  2016   \n",
       "698403592.0              foot fetishes are awesome fuck you     2016   \n",
       "698416945.0     im back   im back and i fully intend to con...  2016   \n",
       "698493321.0     this is for removing my post on 100   im go...  2016   \n",
       "698590313.0                       were not vandalism you idiot  2016   \n",
       "698705636.0   once again im so glad that wikipedia is not b...  2016   \n",
       "698712715.0   holy shit you people suck i dont see any warn...  2016   \n",
       "698763771.0  `   who do you think you are   who died and ma...  2016   \n",
       "698873916.0   stop your propaganda on getting me blocked fo...  2016   \n",
       "698934733.0  ` lol rest assured there is nothing you can ma...  2016   \n",
       "698984149.0     wellgoodbuy isis terroristyou will be block...  2016   \n",
       "699129241.0    you bitch why does it matter its the same go...  2016   \n",
       "699163319.0  `   go fuck yourself    style``backgroundcolor...  2016   \n",
       "699349503.0                   okay king of the wikipedia nazis  2016   \n",
       "699355719.0  ` thank you given the misuse of tools here and...  2016   \n",
       "699445326.0                hey stupydbytch frodesiak i am back  2016   \n",
       "699445375.0                      delete my 200 articles whhore  2016   \n",
       "699447117.0                           you must be an uglybytch  2016   \n",
       "699549435.0  `  motherfucka the whole ``blanqueamento in pu...  2016   \n",
       "699569747.0  ` no you utter retard hispanic means from spai...  2016   \n",
       "699603533.0  `  ``the returds need to get a life`` instead ...  2016   \n",
       "699604332.0  `get a life dhass hole  finding articles 10min...  2016   \n",
       "699645524.0   brandon semenuk has won the event four times ...  2016   \n",
       "699659494.0  im soory since when is google images not allow...  2016   \n",
       "699660419.0  what ever you fuggin fag question how did you ...  2016   \n",
       "699661020.0     nice try but no cigaridiot   then explain o...  2016   \n",
       "699661834.0  `   kys    style``backgroundcolor fdffe7 borde...  2016   \n",
       "699664687.0   shut up mind your own business and go fuck so...  2016   \n",
       "\n",
       "             logged_in       ns   sample  split  label  \n",
       "rev_id                                                  \n",
       "801279.0         False  article   random  train    1.0  \n",
       "2702703.0        False     user   random  train    1.0  \n",
       "4632658.0        False  article  blocked  train    1.0  \n",
       "6545332.0         True     user  blocked  train    1.0  \n",
       "6545351.0         True     user  blocked   test    1.0  \n",
       "7977970.0         True  article   random  train    1.0  \n",
       "8359431.0         True     user  blocked  train    1.0  \n",
       "8724028.0         True     user  blocked   test    1.0  \n",
       "8845700.0        False     user  blocked  train    1.0  \n",
       "8845736.0        False     user  blocked    dev    1.0  \n",
       "9392747.0         True     user  blocked  train    1.0  \n",
       "9396830.0         True     user  blocked    dev    1.0  \n",
       "9521926.0         True     user  blocked    dev    1.0  \n",
       "9664203.0        False     user  blocked  train    1.0  \n",
       "9664254.0        False     user  blocked   test    1.0  \n",
       "9679297.0         True     user  blocked  train    1.0  \n",
       "9725692.0         True     user  blocked  train    1.0  \n",
       "10144987.0        True     user  blocked  train    1.0  \n",
       "10253580.0        True  article  blocked  train    1.0  \n",
       "10255042.0        True  article  blocked  train    1.0  \n",
       "10266382.0       False  article  blocked  train    1.0  \n",
       "10266787.0       False  article  blocked  train    1.0  \n",
       "10266890.0       False     user  blocked  train    1.0  \n",
       "10335102.0       False     user  blocked  train    1.0  \n",
       "10900894.0       False  article  blocked  train    1.0  \n",
       "10949158.0        True     user  blocked  train    1.0  \n",
       "10952727.0       False     user  blocked  train    1.0  \n",
       "10952841.0       False  article  blocked    dev    1.0  \n",
       "11121273.0        True     user  blocked    dev    1.0  \n",
       "11121620.0        True     user  blocked  train    1.0  \n",
       "...                ...      ...      ...    ...    ...  \n",
       "698337564.0      False     user  blocked  train    3.0  \n",
       "698353733.0       True     user   random    dev    3.0  \n",
       "698389664.0      False     user  blocked    dev    3.0  \n",
       "698403592.0      False     user  blocked    dev    3.0  \n",
       "698416945.0      False     user  blocked  train    3.0  \n",
       "698493321.0      False     user  blocked   test    3.0  \n",
       "698590313.0      False     user   random  train    3.0  \n",
       "698705636.0       True  article  blocked  train    3.0  \n",
       "698712715.0       True     user  blocked   test    3.0  \n",
       "698763771.0      False     user  blocked  train    3.0  \n",
       "698873916.0       True     user  blocked  train    3.0  \n",
       "698934733.0       True  article  blocked    dev    3.0  \n",
       "698984149.0      False     user  blocked   test    3.0  \n",
       "699129241.0      False     user  blocked    dev    3.0  \n",
       "699163319.0       True     user  blocked   test    3.0  \n",
       "699349503.0       True     user  blocked  train    3.0  \n",
       "699355719.0       True     user  blocked   test    3.0  \n",
       "699445326.0      False     user  blocked    dev    3.0  \n",
       "699445375.0      False     user  blocked  train    3.0  \n",
       "699447117.0      False     user  blocked  train    3.0  \n",
       "699549435.0       True     user  blocked    dev    3.0  \n",
       "699569747.0      False  article  blocked   test    3.0  \n",
       "699603533.0      False  article  blocked  train    3.0  \n",
       "699604332.0      False     user  blocked    dev    3.0  \n",
       "699645524.0       True     user  blocked    dev    3.0  \n",
       "699659494.0       True     user  blocked   test    3.0  \n",
       "699660419.0       True     user  blocked  train    3.0  \n",
       "699661020.0       True     user  blocked  train    3.0  \n",
       "699661834.0       True     user  blocked  train    3.0  \n",
       "699664687.0       True     user  blocked    dev    3.0  \n",
       "\n",
       "[43734 rows x 7 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments = dataframe.query(\"split=='train'\")\n",
    "valid_comments = dataframe.query(\"split=='test'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   aquafag   if you continue to vandalize my talk page you will be blocked from living on the earth wam   thank you  '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_comments.iloc[100]['comment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train comments length:  26143\n",
      "test comments length:  8789\n"
     ]
    }
   ],
   "source": [
    "print('train comments length: ',len(train_comments))\n",
    "print('test comments length: ',len(valid_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26143,)\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x = train_comments['comment'], valid_comments['comment'], \n",
    "train_y, valid_y = train_comments['label'], valid_comments['label']                                                       \n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'.')\n",
    "count_vect.fit(dataframe['comment'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(dataframe['comment'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,4), max_features=5000)\n",
    "tfidf_vect_ngram.fit(dataframe['comment'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(dataframe['comment'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26143, 5000)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec', encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(dataframe['comment'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, xtrain, ytrain, xvalid, yvalid):\n",
    "    \n",
    "    #predictions = predictions.argmax(axis=-1)\n",
    "   \n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(xvalid)\n",
    "        \n",
    "    accuracy = metrics.accuracy_score(predictions, yvalid)\n",
    "    f1score = metrics.f1_score(yvalid, predictions, average=None)\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:   accuracy: 0.3408806462623734      f1 score: [0.19035337 0.27935144 0.44558378]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count, valid_y)\n",
    "print(\"NB, Count Vectors:   accuracy: %s      f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:   accuracy: 0.3408806462623734      f1 score: [0.19035337 0.27935144 0.44558378]\n",
      "NB, WordLevel TF-IDF:   accuracy: 0.2899078393446353     f1 score: [0.07061713 0.29476966 0.37018837]\n",
      "NB, N-Gram Vectors:   accuracy: 0.31107065650244625     f1 score: [0.06279734 0.27434634 0.41784192]\n",
      "NB, CharLevel Vectors:   accuracy: 0.3168733644328137   f1 score: [0.08637874 0.32567707 0.40128068]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count, valid_y)\n",
    "print(\"NB, Count Vectors:   accuracy: %s      f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"NB, WordLevel TF-IDF:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print(\"NB, N-Gram Vectors:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"NB, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silver\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Silver\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Silver\\AppData\\Roaming\\Python\\Python35\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:   accuracy: 0.3520309477756286   f1 score: [0.00359842 0.16787942 0.50515935]\n",
      "LR, WordLevel TF-IDF:   accuracy: 0.28478780293548756   f1 score: [0.13087157 0.28394186 0.35405307]\n",
      "LR, N-Gram Vectors:   accuracy: 0.3043577198771191   f1 score: [0.10925819 0.25998831 0.40539637]\n",
      "LR, CharLevel Vectors:   accuracy: 0.29775856183866195   f1 score: [0.11820463 0.31378548 0.36556487]\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count, valid_y)\n",
    "print(\"LR, Count Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"LR, WordLevel TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print(\"LR, N-Gram Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"LR, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier on Count Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"SVM, Count Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# SVM Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"SVM, WordLevel TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# SVM Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"SVM, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward NN with 1 hidden layer\n",
    "def model_FF(xtrain, ytrain, xvalid, yvalid, hidden_size, epochs =1):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((xtrain.shape[1], ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(hidden_size, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(3, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "    classifier.fit(xtrain, ytrain,\n",
    "                  batch_size=256,\n",
    "                  epochs=epochs,\n",
    "                  shuffle = True)\n",
    "    # scores of the classifier\n",
    "    scores = classifier.evaluate(xvalid, yvalid, verbose=0)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26143, 3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to one_hot\n",
    "train_y_onehot = keras.utils.to_categorical(train_y, 3)\n",
    "valid_y_onehot = keras.utils.to_categorical(valid_y, 3)\n",
    "train_y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "26143/26143 [==============================] - 3s 100us/step - loss: 1.7157 - acc: 0.3327\n",
      "NN, Count Vectors accuracy:  0.3351917169460028\n",
      "Epoch 1/1\n",
      "26143/26143 [==============================] - 2s 95us/step - loss: 1.0996 - acc: 0.3460: 3\n",
      "NN, Count Vectors accuracy:  0.32108317218769233\n",
      "Epoch 1/1\n",
      "26143/26143 [==============================] - 2s 86us/step - loss: 1.0998 - acc: 0.3429\n",
      "NN, Count Vectors accuracy:  0.34235976789168276\n",
      "Epoch 1/1\n",
      "26143/26143 [==============================] - 4s 162us/step - loss: 1.0990 - acc: 0.3440 2s - loss: 1.0989  - ETA: 1s - loss: 1.\n",
      "NN, Count Vectors accuracy:  0.3456593469109114\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NN Classifier on Count Vectors\n",
    "scores = model_FF(xtrain_count, train_y_onehot, xvalid_count, valid_y_onehot, 100)\n",
    "print(\"NN, Count Vectors accuracy: \", scores[1])\n",
    "\n",
    "# NN Classifier on Word Level TF IDF Vectors\n",
    "scores = model_FF(xtrain_tfidf, train_y_onehot, xvalid_tfidf, valid_y_onehot, 100)\n",
    "print(\"NN, Count Vectors accuracy: \", scores[1])\n",
    "\n",
    "# NN Classifier on Ngram Level TF IDF Vectors\n",
    "scores = model_FF(xtrain_tfidf_ngram, train_y_onehot, xvalid_tfidf_ngram, valid_y_onehot, 100)\n",
    "print(\"NN, Count Vectors accuracy: \", scores[1])\n",
    "\n",
    "# NN Classifier on Count Vectors\n",
    "scores = model_FF(xtrain_tfidf_ngram_chars, train_y_onehot, xvalid_tfidf_ngram_chars, valid_y_onehot, 100)\n",
    "print(\"NN, Count Vectors accuracy: \", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(xtrain, ytrain, xvalid, yvalid, epochs = 10):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=epochs)\n",
    "    scores = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "26143/26143 [==============================] - 42s 2ms/step - loss: 1.1002 - acc: 0.3477\n",
      "Epoch 2/10\n",
      "26143/26143 [==============================] - 37s 1ms/step - loss: 1.0974 - acc: 0.3529\n",
      "Epoch 3/10\n",
      "26143/26143 [==============================] - 37s 1ms/step - loss: 1.0967 - acc: 0.3580\n",
      "Epoch 4/10\n",
      "26143/26143 [==============================] - 36s 1ms/step - loss: 1.0957 - acc: 0.3622\n",
      "Epoch 5/10\n",
      "26143/26143 [==============================] - 36s 1ms/step - loss: 1.0949 - acc: 0.3627\n",
      "Epoch 6/10\n",
      "26143/26143 [==============================] - 36s 1ms/step - loss: 1.0937 - acc: 0.3668\n",
      "Epoch 7/10\n",
      "26143/26143 [==============================] - 37s 1ms/step - loss: 1.0917 - acc: 0.3733\n",
      "Epoch 8/10\n",
      "26143/26143 [==============================] - 36s 1ms/step - loss: 1.0897 - acc: 0.3749\n",
      "Epoch 9/10\n",
      "26143/26143 [==============================] - 36s 1ms/step - loss: 1.0873 - acc: 0.3791\n",
      "Epoch 10/10\n",
      "26143/26143 [==============================] - 36s 1ms/step - loss: 1.0828 - acc: 0.3880\n",
      "CNN, Word Embeddings acuuracy:  0.3161906929353301\n"
     ]
    }
   ],
   "source": [
    "scores = cnn(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"CNN, Word Embeddings acuuracy: \",scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(xtrain, ytrain, xvalid, yvalid, epochs = 3):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(100))(embedding_layer)\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=3)\n",
    "    \n",
    "    scores = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 6144/26143 [======>.......................] - ETA: 2:30 - loss: 0.6373 - acc: 0.6667"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-3f1f03befd58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_seq_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_seq_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y_onehot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LSTM, Word Embeddings:  accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-fde04fa5a08d>\u001b[0m in \u001b[0;36mlstm\u001b[1;34m(xtrain, ytrain, xvalid, yvalid, epochs)\u001b[0m\n\u001b[0;32m     19\u001b[0m     model.fit(xtrain, ytrain,\n\u001b[0;32m     20\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m               epochs=3)\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = lstm(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"LSTM, Word Embeddings:  accuracy: \", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
